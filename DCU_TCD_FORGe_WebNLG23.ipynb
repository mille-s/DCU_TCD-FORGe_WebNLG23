{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3sitW9BFyRWOr9XoeshpT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille-s/DCU_TCD-FORGe_WebNLG23/blob/main/DCU_TCD_FORGe_WebNLG23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0a - Download resources and install Java 8"
      ],
      "metadata": {
        "id": "v7KZ6c4qruNR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1EixVIRHmxqi"
      },
      "outputs": [],
      "source": [
        "# Run this cell to download and unzip the working folder (once run, click \"Refresh\" on the top left corner to see the folders)\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Download FORGe\n",
        "! gdown 1lsh8pwUp9mc0Z_aFbSy1WTIpSx9YwFFD\n",
        "! unzip /content/FORGe_colab_v3.zip\n",
        "\n",
        "# Download Morphology\n",
        "! gdown 1vk1utEjeZ_2YO1H20DPDTjVSevgRJNM_\n",
        "folder_name = 'test_irish_morph_gen_v5.0'\n",
        "zip_name = folder_name+'.zip'\n",
        "! unzip {zip_name}\n",
        "\n",
        "import os\n",
        "morph_input_folder = '/content/'+folder_name+'/Inputs'\n",
        "morph_output_folder = '/content/'+folder_name+'/Outputs'\n",
        "os.makedirs(morph_input_folder)\n",
        "os.makedirs(morph_output_folder)\n",
        "\n",
        "# Make flookup executable\n",
        "! 7z a -sfx {folder_name}'/flookup.exe' {folder_name}'/flookup'\n",
        "! chmod 755 {folder_name}'/flookup'\n",
        "\n",
        "# Clean\n",
        "! rm '/content/FORGe_colab_v3.zip'\n",
        "! rm '/content/test_irish_morph_gen_v5.0.zip'\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to switch to Java 1.8 (needed for FORGe to run correctly)\n",
        "import os\n",
        "def install_java():\n",
        "  !apt-get install -y openjdk-8-jdk-headless -qq > /dev/null      #install openjdk\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"     #set environment variable\n",
        "  !update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "  !java -version       #check java version\n",
        "install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZqqckRKrv3J",
        "outputId": "38fa3172-390f-4c6e-f0b4-5ef85284ee4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "openjdk version \"1.8.0_382\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_382-8u382-ga-1~22.04.1-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.382-b05, mixed mode)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0b - Set parameters"
      ],
      "metadata": {
        "id": "EMbYCGoVPSLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS FOR GENERATION\n",
        "\n",
        "# The input structure(s) of the correct type should be placed in the folder that corresponds to the first module called in the next cell\n",
        "# E.g. if one a module PredArg_... or DSynt_... is selected, the input predicate-argument structures should be placed in the structures/00-PredArg folder\n",
        "# I'll make the instructions and names clearer in a later (actually usable) version.\n",
        "\n",
        "######################################################################\n",
        "\n",
        "# Select language\n",
        "language = 'GA' #@param['EN', 'ES', 'FR', 'GA']\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "# Group consecutive modules for the same system or call each module separately.\n",
        "# Select 'no' to get all intermediate representations, 'yes' if you're only interested in the output.\n",
        "group_modules_prm = 'yes' #@param['yes', 'no']\n",
        "\n",
        "#######################################################################\n",
        "\n",
        "# Modules to run, with type of processing (FORGe, Model1, SimpleNLG, etc.).\n",
        "# Only FORGe is supported for this prototype version.\n",
        "# What if a module spans over several of these?\n",
        "PredArg_Normalisation = 'FORGe'\n",
        "PredArg_AggregationMark = ''\n",
        "PredArg_Aggregation = 'FORGe'\n",
        "PredArg_PoSTagging = 'FORGe'\n",
        "PredArg_CommStructuring = 'FORGe'\n",
        "DSynt_Structuring = 'FORGe'\n",
        "SSynt_Structuring = 'FORGe'\n",
        "SSynt_Aggregation = 'FORGe'\n",
        "RE_Generation = 'FORGe'\n",
        "DMorph_AgreementsLinearisation = 'FORGe'\n",
        "SMorph_Processing = 'FORGe'\n",
        "# Define all micro modules and several higher level modules that can overlap, the highest level being the one-shot generation.\n",
        "#Surface_Generation = 'IMS' # That could take DSynt/SSynt as input and return text; to be defined during the query processing"
      ],
      "metadata": {
        "cellView": "code",
        "id": "sZrDTZNaPWkP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pi9-g7wFiLYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - RDF to PredArg (TBD)"
      ],
      "metadata": {
        "id": "DhPOmwVKtnp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - PredArg to uninflected text (FORGe via M-FleNS pipeline code)"
      ],
      "metadata": {
        "id": "t0l_N2UGtrNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FORGe Static parameters and functions"
      ],
      "metadata": {
        "id": "qLgHKjW5r9OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No need to edit this block for running the generator\n",
        "\n",
        "# The elements of this list are referenced from several places in the code\n",
        "# When updating list, update process_input function!\n",
        "#                 0             1               2                3             4            5            6        7          8        9         10          11\n",
        "level_names = ['PredArg', 'PredArgNorm', 'PredArgAggMark', 'PredArgAgg', 'PredArgPoS', 'PredArgComm', 'DSynt', 'SSynt', 'SSyntAgg', 'REG', 'DMorphLin', 'SMorphText']\n",
        "\n",
        "# Static - Description of RGB modules (FORGe)\n",
        "# Also add dicos here to save some time when loading resources\n",
        "PredArg0_dict = {'input': ['Init'], 'grammars': []}\n",
        "PredArg1_Normalisation_dict = {'input': [level_names[0]], 'grammars': ['10_Con_Sem.rl']}\n",
        "PredArg2_Aggregation_dict = {'input': [level_names[1], level_names[2]], 'grammars': ['11.1_Con_Agg1.rl', '11.2_Con_Agg2.rl', '11.3_Con_Agg3.rl', '11.4_Con_Agg4.rl']}\n",
        "PredArg3_PoSTagging_dict = {'input': [level_names[1], level_names[3]], 'grammars': ['13_Sem_SemPoS.rl']}\n",
        "PredArg4_CommStructuring_dict = {'input': [level_names[4]], 'grammars': ['15_SemPoS_SemCommMark.rl', '17_SemCommMark_SemComm.rl']}\n",
        "DSynt_Structuring_dict = {'input': [level_names[5]], 'grammars': ['20_SemComm_DSynt.rl']}\n",
        "SSynt_Structuring_dict = {'input': [level_names[6]], 'grammars': ['30_DSynt_SSynt.rl', '35_SSynt_PostProc.rl']}\n",
        "SSynt_Aggregation_dict = {'input': [level_names[7]], 'grammars': ['37.1_SSynt_Agg1.rl', '37.2_SSynt_Agg2.rl']}\n",
        "REG_dict = {'input': [level_names[7], level_names[8]], 'grammars': ['38.1_SSynt_REG1.rl', '38.2_SSynt_REG2.rl']}\n",
        "DMorph_AgreementsLinearisation_dict = {'input': [level_names[7], level_names[8], level_names[9]], 'grammars': ['40_SSynt_DMorph_linearize.rl']}\n",
        "SMorph_Processing_dict = {'input': [level_names[10]], 'grammars': ['50_DMorph_SMorph.rl', '60_Smorph_Sentence.rl']}\n",
        "\n",
        "# Static - Description of RGB dicos (FORGe)\n",
        "dic_common_list = ['EN_control.dic', 'concepticon.dic', 'EN_lexicon_MS.dic', 'project_KRISTINA.dic']\n",
        "dic_indep_list = ['language_info.dic', 'lexicon.dic', 'semanticon.dic', 'morphologicon.dic']\n",
        "# If a lexicon.dic, semanticon.dic or morphologicon.dic has something else in the name beyond the language prefix, put in this dictionary\n",
        "dic_special_name_dico = {'EN_lexicon.dic':'EN_lexicon_SMALL.dic'}\n",
        "\n",
        "# Paths to FORGe/MATE folders and property files\n",
        "FORGe_input_folder = '/content/FORGe/buddy_project/struct'\n",
        "path_MATE = '/content/FORGe/buddy-patched.jar'\n",
        "path_props_resources_template = '/content/FORGe/mateColabDrive.properties'\n",
        "path_props_levels = '/content/FORGe/mateLevels.properties'\n",
        "path_props = '/content/FORGe/mate.properties'\n",
        "\n",
        "# Paths to general folders\n",
        "# The input structure(s) of the correct type should be placed in the folder that corresponds to the first module called in the next cell\n",
        "str_PredArg_folder = '/content/FORGe/structures/00-PredArg'\n",
        "str_PredArgNorm_folder = '/content/FORGe/structures/01-PredArgNorm'\n",
        "str_PredArgAggMark_folder = '/content/FORGe/structures/02-PredArgAggMark'\n",
        "str_PredArgAgg_folder = '/content/FORGe/structures/03-PredArgAgg'\n",
        "str_PredArgPoS_folder = '/content/FORGe/structures/04-PredArgPoS'\n",
        "str_PredArgComm_folder = '/content/FORGe/structures/05-PredArgComm'\n",
        "str_DSynt_folder = '/content/FORGe/structures/06-DSynt'\n",
        "str_SSynt_folder = '/content/FORGe/structures/07-SSynt'\n",
        "str_SSyntAgg_folder = '/content/FORGe/structures/08-SSyntAgg'\n",
        "str_REG_folder = '/content/FORGe/structures/09-REG'\n",
        "str_DMorphLin_folder = '/content/FORGe/structures/10-DMorphLin'\n",
        "str_SMorphText_folder = '/content/FORGe/structures/11-SMorphText'\n",
        "\n",
        "class RGBModule:\n",
        "  \"Class to store information related to the RGB modules\"\n",
        "  def __init__(self, module, system, dico_module, in_folder, out_folder):\n",
        "    self.module_type = 'RGB'\n",
        "    self.system = system\n",
        "    self.output = str(module)\n",
        "    self.inputs = dico_module.get('input')\n",
        "    self.grammars = dico_module.get('grammars')\n",
        "    self.input_folder = in_folder\n",
        "    self.output_folder = out_folder\n",
        "\n",
        "def clear_files(folder):\n",
        "  \"Function to clear files from a folder.\"\n",
        "  if os.path.exists(folder) and os.path.isdir(folder):\n",
        "    for filename in os.listdir(folder):\n",
        "      file_path = os.path.join(folder, filename)\n",
        "      try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "          os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "          shutil.rmtree(file_path)\n",
        "      except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "def clear_folder(folder):\n",
        "  \"Function to clear whole folders.\"\n",
        "  if os.path.exists(folder) and os.path.isdir(folder):\n",
        "    for subfolder in os.listdir(folder):\n",
        "      folder_path = os.path.join(folder, subfolder)\n",
        "      if os.path.isdir(folder_path):\n",
        "        try:\n",
        "          shutil.rmtree(folder_path)\n",
        "        except Exception as e:\n",
        "          print('Failed to delete %s. Reason: %s' % (folder, e))\n",
        "\n",
        "def rename_files(folder, output_type):\n",
        "  \"Function that renames files into more a human-friendly format.\"\n",
        "  str_folder_content = os.listdir(folder)\n",
        "  for filename in str_folder_content:\n",
        "    filepath_to_change = os.path.join(folder, filename)\n",
        "    filename_extension = filename.rsplit('.', 1)[1]\n",
        "    filename_noExt = filename.rsplit('.', 1)[0]\n",
        "    clean_filename_noExt = filename_noExt.rsplit('__')[0]\n",
        "    new_filename = clean_filename_noExt+'__'+output_type+'.'+filename_extension\n",
        "    new_filepath = os.path.join(folder, new_filename)\n",
        "    os.rename(filepath_to_change, new_filepath)\n",
        "\n",
        "def copy_files(pathIn, pathOut):\n",
        "  \"Function to copy files into the folder they need to be in to be processed.\"\n",
        "  #print(pathIn)\n",
        "  str_folder_content = os.listdir(pathIn)\n",
        "  for content in str_folder_content:\n",
        "      target_path = os.path.join(pathIn, content)\n",
        "      # If files are found in the folder, copy files to FORGe's input folder\n",
        "      if os.path.isfile(target_path):\n",
        "        shutil.copy(target_path, pathOut)\n",
        "      # If folders are found in the folder, explore them to get to the files\n",
        "      elif os.path.isdir(target_path):\n",
        "        str_subfolder_content = os.listdir(target_path)\n",
        "        for deeper_content in str_subfolder_content:\n",
        "          new_target_path = os.path.join(target_path, deeper_content)\n",
        "          if os.path.isfile(new_target_path):\n",
        "            shutil.copy(new_target_path, pathOut)\n",
        "\n",
        "def check_pipeline(list_modules):\n",
        "  \"Function that finds the first structure to be processed by the pipeline.\"\n",
        "  output_str = []\n",
        "  input_str = []\n",
        "  init_str = []\n",
        "  for module_object in list_modules:\n",
        "    # Output_str is a string\n",
        "    output_str.append(module_object.output)\n",
        "    # Input_str is a list that contains one or more elements\n",
        "    input_str.append(module_object.inputs)\n",
        "  for list_input_level in input_str:\n",
        "    list_seen = []\n",
        "    # For a given module, check for how many input types we find a corresponding structure to process\n",
        "    for unique_input_level in list_input_level:\n",
        "      if unique_input_level in output_str:\n",
        "        list_seen.append(unique_input_level)\n",
        "    # If none of the candidate input types is found, the module is the first to apply in the pipeline\n",
        "    if len(list_seen) == 0:\n",
        "      init_str.append(list_input_level)\n",
        "  # If there is only one list of candidate inputs, that's OK, but if there is none or several, it means the pipeline is wrong.\n",
        "  if len(init_str) == 1:\n",
        "    print('  -> Initial structure is '+str(init_str[0])+'.')\n",
        "    return(init_str[0])\n",
        "  else:\n",
        "    print('! ERROR! Hole in the pipeline, several possible first modules, or mapping not defined in \"process_query\" function for a system ('+str(len(init_str))+' initial representations found: '+str(init_str)+').')\n",
        "    exit()\n",
        "\n",
        "def define_module_sequence(list_modules):\n",
        "  \"Function that creates all possible module sequences and returns the one that include all modules\"\n",
        "  candidate_output_sequence_list = []\n",
        "  # Find first module to apply\n",
        "  for module_object in list_modules:\n",
        "    # At this point we made sure that only one module expects the initial structure type (see check_pipeline function)\n",
        "    if module_object.inputs == list_initial_str:\n",
        "      candidate_output_sequence_list.append([module_object.output])\n",
        "\n",
        "  # Creates all possible sequences of modules and saves them as lists\n",
        "  for candidate_output_sequence in candidate_output_sequence_list:\n",
        "    for module_object in list_modules:\n",
        "      for input_type in module_object.inputs:\n",
        "        if input_type == candidate_output_sequence[-1]:\n",
        "          new_candidate_output_sequence_list = candidate_output_sequence.copy()\n",
        "          new_candidate_output_sequence_list.append(module_object.output)\n",
        "          # Update initial list to keep the loop going\n",
        "          candidate_output_sequence_list.append(new_candidate_output_sequence_list)\n",
        "  # Look for the longest sequence of modules\n",
        "  best_candidate_sequence = []\n",
        "  for candidate_output_sequence in candidate_output_sequence_list:\n",
        "    if len(candidate_output_sequence) > len(best_candidate_sequence):\n",
        "      best_candidate_sequence = candidate_output_sequence\n",
        "  return(best_candidate_sequence)\n",
        "\n",
        "def group_modules(module_sequence, list_modules):\n",
        "  \"Groups consecutive modules that are run by the same system. module_sequence is a list of strings, list_modules a list of objects of class 'module'\"\n",
        "  # Let's make a list that will contain X lists for X different system sub-pipelines; the system name is the first element of each list, the second element is another list that contains the module object.\n",
        "  # grouped_modules looks like this: [['SystemName1', [<Module1-Object>, <Module2-Object>]], ['SystemName2', [<Module3-Object>, <Module4-Object>]]]\n",
        "  grouped_modules = []\n",
        "  # the elements in module_sequence are in order of execution\n",
        "  for output_level in module_sequence:\n",
        "      for module_object in list_modules:\n",
        "          if output_level == module_object.output:\n",
        "            # Check that the first element of the last list is not the same as the currently examined object's system\n",
        "            if len(grouped_modules) > 0:\n",
        "              if len(grouped_modules[-1]) > 0:\n",
        "                if grouped_modules[-1][0] == module_object.system:\n",
        "                  # If the last list has the same system name as the current object, add the object to the group\n",
        "                  grouped_modules[-1][1].append(module_object)\n",
        "                else:\n",
        "                  # Otherwise create the next list for the new system\n",
        "                  grouped_modules.append([module_object.system, [module_object]])\n",
        "              else:\n",
        "                grouped_modules.append([module_object.system, [module_object]])\n",
        "            else:\n",
        "              grouped_modules.append([module_object.system, [module_object]])\n",
        "  return(grouped_modules)\n",
        "\n",
        "def create_grouped_module_objects_FORGe(system_modules):\n",
        "  \"Creates a new object of class RGB module that combines the consecutive modules. system_modules contains at least 2 modules.\"\n",
        "  first_module = system_modules[0]\n",
        "  last_module = system_modules[-1]\n",
        "  grammars_list = []\n",
        "  for module in system_modules:\n",
        "    grammars_list = grammars_list + module.grammars\n",
        "  grouped_dico = {'input': [first_module.inputs], 'grammars': grammars_list}\n",
        "  grouped_module = RGBModule(last_module.output, 'FORGe', grouped_dico, first_module.input_folder, last_module.output_folder)\n",
        "  return(grouped_module)\n",
        "\n",
        "# def check_pipeline(modules, list_init_str):\n",
        "# #  \"Checks if there are holes in the pipeline (i.e. if an expected input structure is not provided by any other module. Is this one usefule now? CF find_initial_structure.\"\n",
        "#   list_outputs = []\n",
        "#   # build list of all outputs pruduced by the different modules\n",
        "#   for module in modules:\n",
        "#     if module.output not in list_outputs:\n",
        "#       list_outputs.append(module.output)\n",
        "#   # check that each module takes as input at least one of the outputs of the other modules\n",
        "#   for module in modules:\n",
        "#     list_input_seen = []\n",
        "#     for input in module.inputs:\n",
        "#       if input in list_outputs or input in list_init_str:\n",
        "#        list_input_seen.append(input)\n",
        "#     if len(list_input_seen) == 0:\n",
        "#      print('ERROR! Incomplete pipeline: there will be no available '+str(input)+' structure (required by '+str(module.output)+' module).')\n",
        "#      exit()\n",
        "\n",
        "def process_query(list_modules, PredArg_Normalisation, PredArg_AggregationMark, PredArg_Aggregation, PredArg_PoSTagging, PredArg_CommStructuring, DSynt_Structuring, SSynt_Structuring, SSynt_Aggregation, RE_Generation, DMorph_AgreementsLinearisation, SMorph_Processing):\n",
        "  \"Function to parse the input parameters and prepare the generation pipeline\"\n",
        "  # For each module, create an object with all the relevant class info (module_name, system, dico_module, in_folder, out_folder)\n",
        "  if PredArg_Normalisation == 'FORGe':\n",
        "    PredArg_Normalisation_RGB = RGBModule(level_names[1], 'FORGe', PredArg1_Normalisation_dict, str_PredArg_folder, str_PredArgNorm_folder)\n",
        "    list_modules.append(PredArg_Normalisation_RGB)\n",
        "  else:\n",
        "    pass\n",
        "  if PredArg_AggregationMark == 'FORGe':\n",
        "    pass\n",
        "  else:\n",
        "    pass\n",
        "  if PredArg_Aggregation == 'FORGe':\n",
        "    PredArg_Aggregation_RGB = RGBModule(level_names[3], 'FORGe', PredArg2_Aggregation_dict, str_PredArgNorm_folder, str_PredArgAgg_folder)\n",
        "    list_modules.append(PredArg_Aggregation_RGB)\n",
        "    if PredArg_PoSTagging == 'FORGe':\n",
        "      PredArg_PoSTagging_RGB = RGBModule(level_names[4], 'FORGe', PredArg3_PoSTagging_dict, str_PredArgAgg_folder, str_PredArgPoS_folder)\n",
        "      list_modules.append(PredArg_PoSTagging_RGB)\n",
        "    elif PredArg_PoSTagging == 'HiddenFORGe':\n",
        "      PredArg_PoSTagging_RGB = RGBModule(level_names[4], 'HiddenFORGe', PredArg3_PoSTagging_dict, str_PredArgAgg_folder, str_PredArgPoS_folder)\n",
        "      list_modules.append(PredArg_PoSTagging_RGB)\n",
        "    else:\n",
        "      pass\n",
        "  elif PredArg_PoSTagging == 'FORGe':\n",
        "    PredArg_PoSTagging_RGB = RGBModule(level_names[4], 'FORGe', PredArg3_PoSTagging_dict, str_PredArgNorm_folder, str_PredArgPoS_folder)\n",
        "    list_modules.append(PredArg_PoSTagging_RGB)\n",
        "  else:\n",
        "    pass\n",
        "  if PredArg_CommStructuring == 'FORGe':\n",
        "    PredArg_CommStructuring_RGB = RGBModule(level_names[5], 'FORGe', PredArg4_CommStructuring_dict, str_PredArgPoS_folder, str_PredArgComm_folder)\n",
        "    list_modules.append(PredArg_CommStructuring_RGB)\n",
        "  elif PredArg_CommStructuring == 'HiddenFORGe':\n",
        "    PredArg_CommStructuring_RGB = RGBModule(level_names[5], 'HiddenFORGe', PredArg4_CommStructuring_dict, str_PredArgPoS_folder, str_PredArgComm_folder)\n",
        "    list_modules.append(PredArg_CommStructuring_RGB)\n",
        "  else:\n",
        "    pass\n",
        "  if DSynt_Structuring == 'FORGe':\n",
        "    DSynt_Structuring_RGB = RGBModule(level_names[6], 'FORGe', DSynt_Structuring_dict, str_PredArgComm_folder, str_DSynt_folder)\n",
        "    list_modules.append(DSynt_Structuring_RGB)\n",
        "  else:\n",
        "    pass\n",
        "  if SSynt_Structuring == 'FORGe':\n",
        "    SSynt_Structuring_RGB = RGBModule(level_names[7], 'FORGe', SSynt_Structuring_dict, str_DSynt_folder, str_SSynt_folder)\n",
        "    list_modules.append(SSynt_Structuring_RGB)\n",
        "  else:\n",
        "    pass\n",
        "  # Agg + REG + Lin\n",
        "  if SSynt_Aggregation == 'FORGe':\n",
        "    SSynt_Aggregation_RGB = RGBModule(level_names[8], 'FORGe', SSynt_Aggregation_dict, str_SSynt_folder, str_SSyntAgg_folder)\n",
        "    list_modules.append(SSynt_Aggregation_RGB)\n",
        "    # Agg + REG + Lin\n",
        "    if RE_Generation == 'FORGe':\n",
        "      REG_RGB = RGBModule(level_names[9], 'FORGe', REG_dict, str_SSyntAgg_folder, str_REG_folder)\n",
        "      list_modules.append(REG_RGB)\n",
        "      # Agg + REG + Lin\n",
        "      if DMorph_AgreementsLinearisation == 'FORGe':\n",
        "        DMorph_AgreementsLinearisation_RGB = RGBModule(level_names[10], 'FORGe', DMorph_AgreementsLinearisation_dict, str_REG_folder, str_DMorphLin_folder)\n",
        "        list_modules.append(DMorph_AgreementsLinearisation_RGB)\n",
        "      else:\n",
        "        pass\n",
        "    # Agg + Lin\n",
        "    elif DMorph_AgreementsLinearisation == 'FORGe':\n",
        "      DMorph_AgreementsLinearisation_RGB = RGBModule(level_names[10], 'FORGe', DMorph_AgreementsLinearisation_dict, str_SSyntAgg_folder, str_DMorphLin_folder)\n",
        "      list_modules.append(DMorph_AgreementsLinearisation_RGB)\n",
        "    else:\n",
        "      pass\n",
        "  # REG + Lin\n",
        "  elif RE_Generation == 'FORGe':\n",
        "    REG_RGB = RGBModule(level_names[9], 'FORGe', REG_dict, str_SSynt_folder, str_REG_folder)\n",
        "    list_modules.append(REG_RGB)\n",
        "    # REG + Lin\n",
        "    if DMorph_AgreementsLinearisation == 'FORGe':\n",
        "      DMorph_AgreementsLinearisation_RGB = RGBModule(level_names[10], 'FORGe', DMorph_AgreementsLinearisation_dict, str_REG_folder, str_DMorphLin_folder)\n",
        "      list_modules.append(DMorph_AgreementsLinearisation_RGB)\n",
        "  # Lin\n",
        "  elif DMorph_AgreementsLinearisation == 'FORGe':\n",
        "    DMorph_AgreementsLinearisation_RGB = RGBModule(level_names[10], 'FORGe', DMorph_AgreementsLinearisation_dict, str_SSynt_folder, str_DMorphLin_folder)\n",
        "    list_modules.append(DMorph_AgreementsLinearisation_RGB)\n",
        "  else:\n",
        "    pass\n",
        "  if SMorph_Processing == 'FORGe':\n",
        "    SMorph_Processing_RGB = RGBModule(level_names[11], 'FORGe', SMorph_Processing_dict, str_DMorphLin_folder, str_SMorphText_folder)\n",
        "    list_modules.append(SMorph_Processing_RGB)\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "def process_files_FORGe(module_object):\n",
        "  \"Function to call FORGe to process an input of level X and create an output of level Y\"\n",
        "  \"The function must be called process_files_SystemName\"\n",
        "\n",
        "  # Erase files from input folder\n",
        "  print('  Cleared input folder...')\n",
        "  clear_files(FORGe_input_folder)\n",
        "\n",
        "  # Generate mate.properties file, which points to the resources to apply\n",
        "  print('  Generated mate.properties file...')\n",
        "  input_folder_gen = module_object.input_folder\n",
        "  output_folder_gen = module_object.output_folder\n",
        "  grammars = module_object.grammars\n",
        "  # Create list of dictionaries for generation\n",
        "  dic_resources_list = []\n",
        "  dic_resources_list.extend(dic_common_list)\n",
        "  # Lang. info, Lexicon, semanticon, morphologicon\n",
        "  # We add the language-specific prefix and check if there's a special name for this dico\n",
        "  for dic_name in dic_indep_list:\n",
        "    lspec_dic_name = language+'_'+dic_name\n",
        "    if lspec_dic_name in dic_special_name_dico:\n",
        "      dic_resources_list.append(dic_special_name_dico.get(lspec_dic_name))\n",
        "    else:\n",
        "      dic_resources_list.append(lspec_dic_name)\n",
        "\n",
        "  # Delete existing property file\n",
        "  if os.path.exists(path_props):\n",
        "    os.remove(path_props)\n",
        "\n",
        "  # Read template to create a new file\n",
        "  prop_template = open(path_props_resources_template, 'r')\n",
        "  lines_prop_template = prop_template.readlines()\n",
        "\n",
        "  # Create a new property file\n",
        "  with codecs.open(path_props, 'w', 'utf-8') as f:\n",
        "      for line in lines_prop_template:\n",
        "        if line.startswith('projectDir='):\n",
        "          project_dir = FORGe_input_folder.rsplit('/', 1)[0]\n",
        "          f.write('projectDir='+str(project_dir)+'\\n')\n",
        "        elif line.startswith('resources='):\n",
        "          f.write('resources=')\n",
        "          x = 0\n",
        "          # All but last dico are followed by a comma\n",
        "          while x < len(dic_resources_list) - 1:\n",
        "            f.write(dic_resources_list[x])\n",
        "            f.write(', ')\n",
        "            x = x + 1\n",
        "          # Last dico is followed by a linebreak\n",
        "          if x == len(dic_resources_list) - 1:\n",
        "            f.write(dic_resources_list[x])\n",
        "            f.write('\\n')\n",
        "        elif line.startswith('ruleSets='):\n",
        "            f.write('ruleSets=')\n",
        "            x = 0\n",
        "            while x < len(grammars) - 1:\n",
        "              f.write(grammars[x])\n",
        "              f.write(', ')\n",
        "              x = x + 1\n",
        "            if x == len(grammars) - 1:\n",
        "              f.write(grammars[x])\n",
        "              f.write('\\n')\n",
        "        elif line.startswith('outputDir='):\n",
        "            f.write('outputDir='+output_folder_gen+'\\n')\n",
        "        elif line.startswith('generateText='):\n",
        "          if output_folder_gen == str_SMorphText_folder:\n",
        "            f.write('generateText=true'+'\\n')\n",
        "          else:\n",
        "            f.write('generateText=false'+'\\n')\n",
        "        else:\n",
        "          f.write(line)\n",
        "  f.close()\n",
        "\n",
        "  # Copy files into input folder\n",
        "  print('  Copied files to input folder...')\n",
        "  copy_files(input_folder_gen, FORGe_input_folder)\n",
        "\n",
        "  # Rename files\n",
        "  print('  Renamed files in input folder...')\n",
        "  rename_files(FORGe_input_folder, module_object.output)\n",
        "\n",
        "  # Run generator\n",
        "  print('  Running module(s)...')\n",
        "  !java -jar {path_MATE} {path_props} {path_props_levels} #-> \"log.txt\"\n",
        "\n",
        "def process_files_HiddenFORGe(module_object):\n",
        "  \"Fake function to test the use of several systems in the pipeline\"\n",
        "  \"The function must be called process_files_SystemName\"\n",
        "  process_files_FORGe(module_object)"
      ],
      "metadata": {
        "id": "hJGzLC7kr7UK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FORGe Main code"
      ],
      "metadata": {
        "id": "Ol7gM-S2sBaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path\n",
        "import subprocess\n",
        "import sys\n",
        "from sys import exit\n",
        "import shutil\n",
        "import codecs\n",
        "import re\n",
        "import random\n",
        "\n",
        "!java -version\n",
        "print('=================================================')\n",
        "\n",
        "# Erase files from output folders\n",
        "print('Clearing output folders...')\n",
        "clear_folder(str_PredArgNorm_folder)\n",
        "clear_folder(str_PredArgAggMark_folder)\n",
        "clear_folder(str_PredArgAgg_folder)\n",
        "clear_folder(str_PredArgPoS_folder)\n",
        "clear_folder(str_PredArgComm_folder)\n",
        "clear_folder(str_DSynt_folder)\n",
        "clear_folder(str_SSynt_folder)\n",
        "clear_folder(str_SSyntAgg_folder)\n",
        "clear_folder(str_REG_folder)\n",
        "clear_folder(str_DMorphLin_folder)\n",
        "clear_folder(str_SMorphText_folder)\n",
        "\n",
        "# Fill list_modules with module descriptions as objects of a particular Class (e.g. RGB), each of them with associated properties (e.g. input/output types and folders, grammars, etc.)\n",
        "print('Preparing generation pipeline...')\n",
        "list_modules = []\n",
        "process_query(list_modules, PredArg_Normalisation, PredArg_AggregationMark, PredArg_Aggregation, PredArg_PoSTagging, PredArg_CommStructuring, DSynt_Structuring, SSynt_Structuring, SSynt_Aggregation, RE_Generation, DMorph_AgreementsLinearisation, SMorph_Processing)\n",
        "\n",
        "# Just to make sure the code doesn't depend on the nice order of the list (currently the order established in the process_query function)\n",
        "random.shuffle(list_modules)\n",
        "\n",
        "# Find which structure the pipeline starts with, and check for holes in pipeline at the same time\n",
        "list_initial_str = check_pipeline(list_modules)\n",
        "print('  -> '+str(len(list_modules))+' modules were selected.')\n",
        "\n",
        "# Find module sequence that includes all required modules in the right order\n",
        "module_sequence = define_module_sequence(list_modules)\n",
        "print('  -> Sequence: '+str(module_sequence))\n",
        "if len(list_modules) == len(module_sequence):\n",
        "  print('  -> The pipeline looks good, proceeding...')\n",
        "else:\n",
        "  print('! Possible ERROR! There are '+str(len(list_modules))+' modules selected and the longest module sequence found is '+str(len(module_sequence))+'.')\n",
        "\n",
        "# Break pipeline by system, so we can make one call per system instead of one call per module.\n",
        "modules_to_process = group_modules(module_sequence, list_modules)\n",
        "\n",
        "# Run each (sequence of) module(s) with the desired system\n",
        "# One instance of modules to process looks like that: [['SystemName1', [<Module1-Object>, <Module2-Object>]], ['SystemName2', [<Module3-Object>, <Module4-Object>]], ...]\n",
        "# globals()[function_name] allow for calling a function using a string\n",
        "for system_modules in modules_to_process:\n",
        "  function_name = 'process_files_'+system_modules[0]\n",
        "  print('--------------------------')\n",
        "  print('Running '+system_modules[0])\n",
        "  print('--------------------------')\n",
        "  if len(system_modules[1]) > 1:\n",
        "    if group_modules_prm == 'yes':\n",
        "      grouped_module_object = create_grouped_module_objects_FORGe(system_modules[1])\n",
        "      globals()[function_name](grouped_module_object)\n",
        "    else:\n",
        "      for module_object in system_modules[1]:\n",
        "        print(\"Processing module #\"+str(system_modules[1].index(module_object)))\n",
        "        globals()[function_name](module_object)\n",
        "  else:\n",
        "    globals()[function_name](system_modules[1][0])\n",
        "\n",
        "print('=================================================')\n",
        "print('All done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPOfSXMWsI8R",
        "outputId": "41777715-81d8-48d1-8a66-3e7f0f7223c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_382\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_382-8u382-ga-1~22.04.1-b05)\n",
            "OpenJDK 64-Bit Server VM (build 25.382-b05, mixed mode)\n",
            "=================================================\n",
            "Clearing output folders...\n",
            "Preparing generation pipeline...\n",
            "  -> Initial structure is ['PredArg'].\n",
            "  -> 10 modules were selected.\n",
            "  -> Sequence: ['PredArgNorm', 'PredArgAgg', 'PredArgPoS', 'PredArgComm', 'DSynt', 'SSynt', 'SSyntAgg', 'REG', 'DMorphLin', 'SMorphText']\n",
            "  -> The pipeline looks good, proceeding...\n",
            "--------------------------\n",
            "Running FORGe\n",
            "--------------------------\n",
            "  Cleared input folder...\n",
            "  Generated mate.properties file...\n",
            "  Copied files to input folder...\n",
            "  Renamed files in input folder...\n",
            "  Running module(s)...\n",
            "Loading files...\n",
            "Parsing 10_Con_Sem.rl...\n",
            "Parsing 11.1_Con_Agg1.rl...\n",
            "Parsing 11.2_Con_Agg2.rl...\n",
            "Parsing 11.3_Con_Agg3.rl...\n",
            "Parsing 11.4_Con_Agg4.rl...\n",
            "Parsing 13_Sem_SemPoS.rl...\n",
            "Parsing 15_SemPoS_SemCommMark.rl...\n",
            "Parsing 17_SemCommMark_SemComm.rl...\n",
            "Parsing 20_SemComm_DSynt.rl...\n",
            "Parsing 30_DSynt_SSynt.rl...\n",
            "Parsing 35_SSynt_PostProc.rl...\n",
            "Parsing 37.1_SSynt_Agg1.rl...\n",
            "Parsing 37.2_SSynt_Agg2.rl...\n",
            "Parsing 38.1_SSynt_REG1.rl...\n",
            "Parsing 38.2_SSynt_REG2.rl...\n",
            "Parsing 40_SSynt_DMorph_linearize.rl...\n",
            "Parsing 50_DMorph_SMorph.rl...\n",
            "Parsing 60_Smorph_Sentence.rl...\n",
            "Parsing EN_control.dic...\n",
            "Parsing concepticon.dic...\n",
            "Parsing EN_lexicon_MS.dic...\n",
            "Parsing project_KRISTINA.dic...\n",
            "Parsing GA_language_info.dic...\n",
            "Parsing GA_lexicon.dic...\n",
            "Parsing GA_semanticon.dic...\n",
            "Parsing GA_morphologicon.dic...\n",
            "Setting up the execution...\n",
            "Processing file 230823_dev_4tr_sml_GA__SMorphText.conll...\n",
            "Processing graph ConllSentence0...\n",
            "Processing graph ConllSentence1...\n",
            "Processing graph ConllSentence2...\n",
            "Processing graph ConllSentence3...\n",
            "Processing graph ConllSentence4...\n",
            "Processing graph ConllSentence5...\n",
            "Processing graph ConllSentence6...\n",
            "Processing graph ConllSentence7...\n",
            "Processing graph ConllSentence8...\n",
            "Input graph processed. Result wrote to /content/FORGe/structures/11-SMorphText/230823_dev_4tr_sml_GA__SMorphText.conll/230823_dev_4tr_sml_GA__SMorphText.conll_out.str\n",
            "Text extracted. Result wrote to /content/FORGe/structures/11-SMorphText/230823_dev_4tr_sml_GA__SMorphText.conll/230823_dev_4tr_sml_GA__SMorphText.conll_out.txt\n",
            "=================================================\n",
            "All done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FORGe check outputs and copy to Morph input folder"
      ],
      "metadata": {
        "id": "LMrA9SMWsZDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_folder = '/content/FORGe/structures/00-PredArg'\n",
        "output_folder = '/content/FORGe/structures/11-SMorphText'\n",
        "input_folder_morph = '/content/test_irish_morph_gen_v5.0/FORGe-out'\n",
        "\n",
        "str_count_perLevel = []\n",
        "txt_count_perLevel = []\n",
        "\n",
        "def count_conll(filePath):\n",
        "  \"\"\" counts how many conll structures are in a file \"\"\"\n",
        "  counter = 0\n",
        "  fd = codecs.open(filePath, 'r', 'utf-8')\n",
        "  lines = fd.readlines()\n",
        "  for line in lines:\n",
        "    if re.search('^0\\t_', line):\n",
        "      counter += 1\n",
        "  return(counter)\n",
        "\n",
        "def count_txt(filePath):\n",
        "  \"\"\" counts how many texts are in a file \"\"\"\n",
        "  fd = codecs.open(filePath, 'r', 'utf-8')\n",
        "  lines = fd.readlines()\n",
        "  return(len(lines))\n",
        "\n",
        "def examine_files(path, count_perLevel):\n",
        "  folder_content = os.listdir(path)\n",
        "  # Sorting the files so they remain aligned with the outputs\n",
        "  for file in sorted(folder_content):\n",
        "    file_path = os.path.join(path, file)\n",
        "    # If files are found in the folder, process them\n",
        "    # Get number of input structures\n",
        "    if os.path.isfile(file_path):\n",
        "      if re.search('\\.conll', file_path):\n",
        "        count = count_conll(file_path)\n",
        "        count_perLevel.append(count)\n",
        "    # If folders are found in the folder, go deeper\n",
        "    elif os.path.isdir(file_path):\n",
        "      str_subfolder_content = os.listdir(file_path)\n",
        "      # Sorting the folders so they remain aligned with the inputs\n",
        "      for deeper_content in sorted(str_subfolder_content):\n",
        "        new_file_path = os.path.join(file_path, deeper_content)\n",
        "        # Get number of texts\n",
        "        if os.path.isfile(new_file_path):\n",
        "          if re.search('\\.txt', new_file_path):\n",
        "            count = count_txt(new_file_path)\n",
        "            count_perLevel.append(count)\n",
        "            shutil.copy(new_file_path, input_folder_morph)\n",
        "\n",
        "examine_files(input_folder, str_count_perLevel)\n",
        "examine_files(output_folder, txt_count_perLevel)\n",
        "\n",
        "\n",
        "if str_count_perLevel == txt_count_perLevel:\n",
        "  print('OK!\\n----')\n",
        "else:\n",
        "  print('Problem!\\n----')\n",
        "\n",
        "print('Inputs:  ' + str(sum(str_count_perLevel)))\n",
        "print('Outputs: ' + str(sum(txt_count_perLevel)))\n",
        "\n",
        "print('Inputs:  ' + str(str_count_perLevel))\n",
        "print('Outputs: ' + str(txt_count_perLevel))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdGPU-idses5",
        "outputId": "b10fe68e-4a91-4528-eb47-ee5b453f0bdf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n",
            "----\n",
            "Inputs:  9\n",
            "Outputs: 9\n",
            "Inputs:  [9]\n",
            "Outputs: [9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Morphology processing (Irish NLP Tools)"
      ],
      "metadata": {
        "id": "0iPz_wQhte3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process raw FORGe output"
      ],
      "metadata": {
        "id": "Nmmyp6FtuUo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import codecs\n",
        "import re\n",
        "import os\n",
        "\n",
        "# I copy the FORGe outputs manually to this file\n",
        "# filepath = os.path.join(folder_name,'aerfort_gen_test.txt')\n",
        "# filepath = os.path.join(folder_name,'test_triples_en_ga_utf8_0000-0449.conll_out.txt')\n",
        "# filepath = os.path.join(folder_name,'test_triples_en_ga_utf8_0450-0899.conll_out.txt')\n",
        "# filepath = os.path.join(folder_name,'test_triples_en_ga_utf8_0900-1349.conll_out.txt')\n",
        "# filepath = os.path.join(folder_name,'test_triples_en_ga_utf8_1350-1778.conll_out.txt')\n",
        "\n",
        "def process_FORGe(filepath, count_strs_all):\n",
        "  filename = filepath.rsplit('/', 1)[1]\n",
        "  print('Processing '+filename)\n",
        "  lines = codecs.open(filepath, 'r', 'utf-8').readlines()\n",
        "  fo = codecs.open(os.path.join(morph_input_folder, filename), 'w', 'utf-8')\n",
        "  for line in lines:\n",
        "    # Replace apostrophes/parentheses by something else, they make morph break\n",
        "    line = re.subn(\"'\", '_APSTR_', line)[0]\n",
        "    line = re.subn(\"\\(\", '_OBRKT_', line)[0]\n",
        "    line = re.subn(\"\\)\", '_CBRKT_', line)[0]\n",
        "    line = re.subn(\"&\", '_AMPRS_', line)[0]\n",
        "    line = re.subn(\";\", '_SEMICOL_', line)[0]\n",
        "    line = re.subn(\"\\$\", '_DOLLSIGN_', line)[0]\n",
        "    line = re.subn(\"\\+\", '_PLUSSIGN_', line)[0]\n",
        "    line = re.subn('\\\"', '_DBLQUOT_', line)[0]\n",
        "    # clean quotes that MATE can’t take care of\n",
        "    line = re.subn('\\\\\\\\\"([^\\\\\\\\]+)\\\\\\\\', '\\\\\"\\g<1>\\\\\"', line)[0]\n",
        "    # In FORGe I use % as separator, we need +\n",
        "    line = re.subn('%', '+', line)[0]\n",
        "    # Insert linebreak after each final sentence to separate texts from each other (one text can have several sentences)\n",
        "    line = re.subn(' \\.$', ' .\\n', line)[0]\n",
        "    # Put each word on one line\n",
        "    line = re.subn(' ', '\\n', line)[0]\n",
        "    # Copy to morph gen input folder\n",
        "    fo.write(line)\n",
        "  fo.close()\n",
        "  count_strs_all.append(len(lines))\n",
        "\n",
        "# list_filepaths = glob.glob(os.path.join(folder_name, 'FORGe-out', '*conll_out.txt'))\n",
        "list_filepaths = glob.glob(os.path.join(folder_name, 'FORGe-out', '*.txt'))\n",
        "\n",
        "# To store how many texts we have in each file (used to )\n",
        "count_strs_all_FORGe = []\n",
        "\n",
        "for filepath in list_filepaths:\n",
        "  process_FORGe(filepath, count_strs_all_FORGe)\n",
        "\n",
        "# Check\n",
        "print('\\nThere are '+str(sum(count_strs_all_FORGe))+' texts.')\n",
        "print(count_strs_all_FORGe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd4UYBJLuYJo",
        "outputId": "e1d67afa-9706-487f-d5e3-5547666e38fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 230823_dev_4tr_sml_GA__SMorphText.conll_out.txt\n",
            "\n",
            "There are 9 texts.\n",
            "[9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Call morph generator"
      ],
      "metadata": {
        "id": "mvnzG_dquaFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# v3 (fast, ~2sec/450 texts)\n",
        "\n",
        "# Run the morphology generation\n",
        "from IPython.display import HTML, display\n",
        "import progressbar\n",
        "import glob\n",
        "import codecs\n",
        "from termcolor import colored\n",
        "\n",
        "show_input = False #@param {type:\"boolean\"}\n",
        "\n",
        "# To store how many texts we have in each file (used to )\n",
        "count_strs_all_Morph = []\n",
        "\n",
        "for filepath in glob.glob(os.path.join(morph_input_folder, '*.*')):\n",
        "  count_strs_all = 0\n",
        "  head, tail = os.path.split(filepath)\n",
        "  filename = tail.rsplit('.')[0]\n",
        "  print('Processing '+filename)\n",
        "  fo = codecs.open(morph_output_folder+'/'+filename+'_out.txt', 'w', 'utf-8')\n",
        "  list_inflected_words = ! cat {filepath} | {folder_name}'/flookup' -a {folder_name}'/allgen.fst'\n",
        "  # print(list_inflected_words)\n",
        "\n",
        "  # Create a variable to store the outputs\n",
        "  text = ''\n",
        "  # morph returns this as list_inflected_words: ['imir+Verb+Vow+PresInd\\timríonn', '', 'Agremiação_Sportiva_Arapiraquense+Noun+Masc+Com+Pl\\t+?', '', ',\\t+?',...]\n",
        "  for word in list_inflected_words:\n",
        "    empty = 'yes'\n",
        "    input_string = ''\n",
        "    morph_returned = ''\n",
        "    morph_backup = ''\n",
        "    if re.search('\\t', word):\n",
        "      # for every space an empty string is returned; we'll ignore them later. Between two consecutive texts there is a simple \"\\t\" with nothing around. I use this to introduce linebreaks later.\n",
        "      empty = 'no'\n",
        "      input_string = word.split('\\t')[0]\n",
        "      morph_returned = word.split('\\t')[1]\n",
        "      if re.search('\\+', word):\n",
        "        morph_backup = input_string.split('+', 1)[0]\n",
        "      else:\n",
        "        morph_backup = input_string\n",
        "    out_line = ''\n",
        "    # Create each output line with the required contents\n",
        "    if show_input == True:\n",
        "      if empty == 'no':\n",
        "        if morph_returned == '':\n",
        "          if input_string == '':\n",
        "            out_line = out_line + '\\n'\n",
        "            count_strs_all += 1\n",
        "        else:\n",
        "          out_line = out_line + input_string + ': ' +'\\x1b[5;30;47m'+morph_returned+'\\x1b[0m'+'\\n'\n",
        "    else:\n",
        "      if empty == 'no':\n",
        "        if morph_returned == '+?':\n",
        "          out_line = out_line + morph_backup + ' '\n",
        "        # If the line is empty, add a line break (empty lines separate different texts in the input)\n",
        "        elif morph_returned == '':\n",
        "          if input_string == '':\n",
        "            out_line = out_line + '\\n'\n",
        "            count_strs_all += 1\n",
        "        else:\n",
        "          out_line = out_line + morph_returned + ' '\n",
        "    # add line to the other lines of the same file\n",
        "    text = text + out_line\n",
        "\n",
        "  # print('\\n----------------------\\n'+text+'\\n')\n",
        "  count_strs_all_Morph.append(count_strs_all)\n",
        "  fo.write(text+'\\n')\n",
        "  fo.close()\n",
        "\n",
        "# Check\n",
        "print('\\nThere are '+str(sum(count_strs_all_Morph))+' texts.')\n",
        "print(count_strs_all_Morph)\n",
        "if not sum(count_strs_all_Morph) == sum(count_strs_all_FORGe):\n",
        "  print('\\nERROR! Mismatch with FORGe outputs!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "code",
        "id": "uPBqNqjkueAk",
        "outputId": "4cc43807-a1ba-446d-84e7-4435eb154534"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 230823_dev_4tr_sml_GA__SMorphText\n",
            "\n",
            "There are 9 texts.\n",
            "[9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p0sD7xNXhw9N"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - Output post-processing and packaging"
      ],
      "metadata": {
        "id": "MSsBqWCAuxyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean output text\n",
        "\n",
        "import glob\n",
        "import sys\n",
        "import os\n",
        "import codecs\n",
        "import re\n",
        "\n",
        "#encodings: cp1252, utf-8, utf-16BE ...\n",
        "\n",
        "#get path of input folder\n",
        "containing_path = morph_output_folder.rsplit('\\\\', 1)[0].replace('\\\\', '/')\n",
        "language = 'GA'\n",
        "with_underscores = 'no'\n",
        "without_underscores = 'yes'\n",
        "#folder_name = path.rsplit('\\\\', 1)[1]\n",
        "\n",
        "#filename = os.path.join(containing_path, str(x)+'_report.txt')\n",
        "\n",
        "#create output file\n",
        "#fo = codecs.open(filename, 'a', 'utf-8')\n",
        "\n",
        "#read filepaths\n",
        "#list_filepaths = glob.glob(os.path.join(path, '*.conll'))\n",
        "list_filepaths = glob.glob(os.path.join(morph_output_folder, '*_out.txt'))\n",
        "\n",
        "def uppercase(match):\n",
        "  return(match.group(1).upper())\n",
        "\n",
        "def uppercaseDot(match):\n",
        "  return('. '+match.group(1).upper())\n",
        "\n",
        "def clean_outputs (text, count, underscores):\n",
        "  # Irish things\n",
        "  if language == 'GA':\n",
        "    # Reestablish punctuations removed for morph processing\n",
        "    text = re.subn(\"_APSTR_\", \"'\", text)[0]\n",
        "    text = re.subn(\"_OBRKT_\", \"(\", text)[0]\n",
        "    text = re.subn(\"_CBRKT_\", \")\", text)[0]\n",
        "    text = re.subn(\"_AMPRS_\", \"&\", text)[0]\n",
        "    text = re.subn(\"_SEMICOL_\", \";\", text)[0]\n",
        "    text = re.subn(\"_DOLLSIGN_\", \"$\", text)[0]\n",
        "    text = re.subn(\"_PLUSSIGN_\", \"+\", text)[0]\n",
        "    text = re.subn(\"_DBLQUOT_\", '\\\"', text)[0]\n",
        "    # Remove tig/thig (bug morph, fixed)\n",
        "    # text = re.subn(\"tagann tig \", \"tagann \", text)[0]\n",
        "    # text = re.subn(\"thagann thig \", \"thagann \", text)[0]\n",
        "    # Process prefixes (either after a space or at the beginning of a line)\n",
        "    text = re.subn(\" d- \", \" d'\", text)[0]\n",
        "    text = re.subn(\"^d- \", \"d'\", text)[0]\n",
        "    text = re.subn(' h- ', ' h', text)[0]\n",
        "    text = re.subn('^h- ', 'h', text)[0]\n",
        "    text = re.subn(' t- ', ' t-', text)[0]\n",
        "    text = re.subn('^t- ', 't-', text)[0]\n",
        "    # The hyphens left should all be from prefixed adjectives\n",
        "    text = re.subn('- - ', '', text)[0]\n",
        "    text = re.subn(' - ', '', text)[0]\n",
        "    text = re.subn('- ', '', text)[0]\n",
        "    # Bring together a + tá in relative clauses\n",
        "    text = re.subn(' a tá ', ' atá ', text)[0]\n",
        "    # contract \"de\" in front of \"fh\" and vowels, i, etc.\n",
        "    text = re.subn(' de [aA]n ', ' den ', text)[0]\n",
        "    text = re.subn(' de [aA]n_', ' den ', text)[0]\n",
        "    text = re.subn(' do [aA]n ', ' don ', text)[0]\n",
        "    text = re.subn(' do [aA]n_', ' don ', text)[0]\n",
        "    text = re.subn(' faoi [aA]n ', ' faoin ', text)[0]\n",
        "    text = re.subn(' faoi [aA]n_', ' faoin ', text)[0]\n",
        "    text = re.subn(' ó [aA]n ', ' ón ', text)[0]\n",
        "    text = re.subn(' ó [aA]n_', ' ón ', text)[0]\n",
        "    text = re.subn(\" d[eo] ([fF])h\", \" d'\\g<1>h\", text)[0]\n",
        "    text = re.subn(\" de ([aeiouAEIOUáéíóúÁÉÍÓÚ])\", \" d'\\g<1>\", text)[0]\n",
        "    text = re.subn(' i Éire ', ' in Éirinn ', text)[0]\n",
        "    text = re.subn(' i An ', ' ins An ', text)[0]\n",
        "    text = re.subn(' i An_', ' ins An ', text)[0]\n",
        "    text = re.subn(' i ([aeiouáéíóúAEIOUÁÉÍÓÚ])', ' in \\g<1>', text)[0]\n",
        "    # Lenition f (+ contraction)\n",
        "    text = re.subn(' d[eo] ([fF])([^h])', \" d'\\g<1>h\\g<2>\", text)[0]\n",
        "    text = re.subn(\" (ar|de|do|faoi|mar|ó|roimh|trí|um|céad) ([bcdfgmptBCDFGMPT])([^hH])\", \" \\g<1> \\g<2>h\\g<3>\", text)[0]\n",
        "    text = re.subn(\" (ar|de|do|faoi|mar|ó|roimh|trí|um|céad) ([sS])([^hcfmptvHCFMPTV])\", \" \\g<1> \\g<2>h\\g<3>\", text)[0]\n",
        "    # Eclipsis\n",
        "    text = re.subn(' i ([bB][^pP])', ' i m\\g<1>', text)[0]\n",
        "    text = re.subn(' i ([cC])', ' i g\\g<1>', text)[0]\n",
        "    text = re.subn(' i ([dD][^tT])', ' i n\\g<1>', text)[0]\n",
        "    text = re.subn(' i ([fF])', ' i bh\\g<1>', text)[0]\n",
        "    text = re.subn(' i ([gG][^cC])', ' i n\\g<1>', text)[0]\n",
        "    text = re.subn(' i ([pP])', ' i b\\g<1>', text)[0]\n",
        "    text = re.subn(' i ([tT])', ' i d\\g<1>', text)[0]\n",
        "    # text = re.subn(' i ([aeiouáéíóú])', ' i n-\\g<1>', text)[0]\n",
        "    # text = re.subn(' i ([AEIOUÁÉÍÓÚ])', ' i n\\g<1>', text)[0]\n",
        "    # Ugly patches\n",
        "    text = re.subn(' ar bhí ', ' a bhí ', text)[0]\n",
        "    text = re.subn(' an ann ', ' air ', text)[0]\n",
        "    text = re.subn(' an sé ', ' an é ', text)[0]\n",
        "  # Erroneous \"type = parenthetical\" fix\n",
        "  if re.search('\\) \\(', text):\n",
        "    print('!!! Failed parenthesis generation in input '+str(count)+' (fixed)')\n",
        "    text = re.subn('\\) \\(', ' ', text)[0]\n",
        "  # clean quotes that MATE can’t take care of\n",
        "  text = re.subn('\\\\\\\\\"([^\\\\\\\\]+)\\\\\\\\', '\"\\g<1>\"', text)[0]\n",
        "  # uppercase words when at the beginning of a sentence or after a final dot (dots preceded by a space)\n",
        "  text = re.subn(' \\. ([a-z])', uppercaseDot, text)[0]\n",
        "  text = re.subn('^([a-z])', uppercase, text)[0]\n",
        "  # replace \"a\" by \"an\" before vowels (should restrict to English)\n",
        "  if language == 'EN':\n",
        "    text = re.subn(' a ([aeio])', ' an \\g<1>', text)[0]\n",
        "  # find generation fails (we introduce [..] or [...] or [......] when a sentence cannot be generated\n",
        "  if re.search('\\[\\.\\.', text):\n",
        "    print('!!! Failed sentence generation in input '+str(count))\n",
        "  # remove space before commas and dots\n",
        "  text = re.subn(' ,', ',', text)[0]\n",
        "  text = re.subn(' \\.', '.', text)[0]\n",
        "  # replace double dots by single ones\n",
        "  while re.search('\\.\\.', text):\n",
        "    text = re.subn('\\.\\.', '.', text)[0]\n",
        "  text = re.subn('% %', '%', text)[0]\n",
        "  # Clean remnants of non-generated sentences\n",
        "  text = re.subn('Sentence \\[\\.\\]\\.', '', text)[0]\n",
        "  # Remove initial spaces\n",
        "  while re.search('^ ', text):\n",
        "    text = re.subn('^ ', '', text)[0]\n",
        "  # replace underscores by spaces\n",
        "  if underscores == 'yes':\n",
        "    pass\n",
        "  else:\n",
        "    text = re.subn('_', ' ', text)[0]\n",
        "  # New 2023: replace double spaces by single ones\n",
        "  while re.search('  ', text):\n",
        "    text = re.subn('  ', ' ', text)[0]\n",
        "  # reformat date/time\n",
        "  text = re.subn('([0-9]+)-([0-9]+)-([0-9]+)T([0-9]+:[0-9]+:[0-9]+)Z', '\\g<1>/\\g<2>/\\g<3> at \\g<4>', text)[0]\n",
        "  return(text)\n",
        "\n",
        "count_strs_all_postproc = []\n",
        "for filepath in list_filepaths:\n",
        "  count_strs_all = 0\n",
        "  # filename = filepath.split('\\\\')[-1].split('.')[0]\n",
        "  head, tail = os.path.split(filepath)\n",
        "  filename = tail.rsplit('.')[0]\n",
        "  fd = codecs.open(filepath, 'r', 'utf-8')\n",
        "  print('Processing '+filename)\n",
        "  filename_out_noUnderscores = filename+'_postproc''.txt'\n",
        "  filename_out_Underscores = filename+'_postproc_underscores''.txt'\n",
        "  if without_underscores == 'yes':\n",
        "    fo1 = codecs.open(os.path.join(morph_output_folder, filename_out_noUnderscores), 'w', 'utf-8')\n",
        "  if with_underscores == 'yes':\n",
        "    fo2 = codecs.open(os.path.join(morph_output_folder, filename_out_Underscores), 'w', 'utf-8')\n",
        "  lines = fd.readlines()\n",
        "  x = 0\n",
        "  for line in lines:\n",
        "    # To filter out final linebreak in each file\n",
        "    if not line == '\\n':\n",
        "      if without_underscores == 'yes':\n",
        "        new_line = clean_outputs(line, x, 'no')\n",
        "        fo1.write(new_line)\n",
        "      if with_underscores == 'yes':\n",
        "        new_line = clean_outputs(line, x, 'yes')\n",
        "        fo2.write(new_line)\n",
        "      count_strs_all += 1\n",
        "    x += 1\n",
        "  if without_underscores == 'yes':\n",
        "    fo1.close()\n",
        "  if with_underscores == 'yes':\n",
        "    fo2.close()\n",
        "  count_strs_all_postproc.append(count_strs_all)\n",
        "\n",
        "# Check\n",
        "print('\\nThere are '+str(sum(count_strs_all_postproc))+' texts.')\n",
        "print(count_strs_all_postproc)\n",
        "if not sum(count_strs_all_postproc) == sum(count_strs_all_FORGe):\n",
        "  print('\\nERROR! Mismatch with FORGe outputs!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQuFTzReu2KV",
        "outputId": "ddb73fb8-3f7f-4207-8d9d-a8ffd3deb3ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 230823_dev_4tr_sml_GA__SMorphText_out\n",
            "\n",
            "There are 9 texts.\n",
            "[9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate files\n",
        "\n",
        "list_clean_outputs = glob.glob(os.path.join(morph_output_folder, '*_out_postproc.txt'))\n",
        "filename = 'all_out.txt'\n",
        "\n",
        "with codecs.open(filename, 'w', 'utf-8') as outfile:\n",
        "  # Files need to be sorted to be concatenated in the right order\n",
        "  for fname in sorted(list_clean_outputs):\n",
        "    print('Processing '+fname)\n",
        "    with open(fname) as infile:\n",
        "      outfile.write(infile.read())\n",
        "\n",
        "# Check\n",
        "count_texts_all = len(codecs.open(filename).readlines())\n",
        "print('\\nThere are '+str(count_texts_all)+' texts.')\n",
        "if not count_texts_all == sum(count_strs_all_FORGe):\n",
        "  print('\\nERROR! Mismatch with FORGe outputs!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfQ08wHOu8eZ",
        "outputId": "35915e33-aeb4-4340-f6ee-04501edb3d59"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/test_irish_morph_gen_v5.0/Outputs/230823_dev_4tr_sml_GA__SMorphText_out_postproc.txt\n",
            "\n",
            "There are 9 texts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zip output folder to download"
      ],
      "metadata": {
        "id": "A_5IopOGsazQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split = \"ukn\" #@param['dev', 'test','train','ukn']\n",
        "!zip -r /content/WebNLG_[{split}]_allLevels.zip /content/FORGe/structures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_yzCQGBshN1",
        "outputId": "6b5fc6d6-a1d5-40b9-c5cd-cf58bcf650f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/FORGe/structures/ (stored 0%)\n",
            "  adding: content/FORGe/structures/08-SSyntAgg/ (stored 0%)\n",
            "  adding: content/FORGe/structures/01-PredArgNorm/ (stored 0%)\n",
            "  adding: content/FORGe/structures/03-PredArgAgg/ (stored 0%)\n",
            "  adding: content/FORGe/structures/00-PredArg/ (stored 0%)\n",
            "  adding: content/FORGe/structures/00-PredArg/230823_dev_4tr_sml_GA.conll (deflated 88%)\n",
            "  adding: content/FORGe/structures/05-PredArgComm/ (stored 0%)\n",
            "  adding: content/FORGe/structures/06-DSynt/ (stored 0%)\n",
            "  adding: content/FORGe/structures/11-SMorphText/ (stored 0%)\n",
            "  adding: content/FORGe/structures/11-SMorphText/230823_dev_4tr_sml_GA__SMorphText.conll/ (stored 0%)\n",
            "  adding: content/FORGe/structures/11-SMorphText/230823_dev_4tr_sml_GA__SMorphText.conll/230823_dev_4tr_sml_GA__SMorphText.conll_out.txt (deflated 79%)\n",
            "  adding: content/FORGe/structures/11-SMorphText/230823_dev_4tr_sml_GA__SMorphText.conll/230823_dev_4tr_sml_GA__SMorphText.conll_out.str (deflated 84%)\n",
            "  adding: content/FORGe/structures/09-REG/ (stored 0%)\n",
            "  adding: content/FORGe/structures/07-SSynt/ (stored 0%)\n",
            "  adding: content/FORGe/structures/04-PredArgPoS/ (stored 0%)\n",
            "  adding: content/FORGe/structures/10-DMorphLin/ (stored 0%)\n",
            "  adding: content/FORGe/structures/02-PredArgAggMark/ (stored 0%)\n"
          ]
        }
      ]
    }
  ]
}